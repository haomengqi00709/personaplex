"""
PersonaPlex å®æ—¶ç¿»è¯‘ - äº‘ç«¯ GPU ç‰ˆæœ¬ï¼ˆä¿æŒæ¨¡å‹çŠ¶æ€ï¼‰
åœ¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹ä¸€æ¬¡ï¼Œä¿æŒçŠ¶æ€ï¼Œé¿å…æ¯æ¬¡é‡æ–°åŠ è½½
"""

from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from flask_socketio import SocketIO, emit
import torch
import numpy as np
import soundfile as sf
import librosa
import os
import warnings
import tempfile
import base64
import threading
import gc
import queue
import time
import sentencepiece

warnings.filterwarnings("ignore")

# è®¾ç½® PyTorch CUDA å†…å­˜åˆ†é…é…ç½®ï¼Œå‡å°‘å†…å­˜ç¢ç‰‡
os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')

# æ£€æŸ¥å¹¶è®¾ç½® Hugging Face Token
if not os.environ.get('HF_TOKEN'):
    print("âš ï¸  è­¦å‘Š: HF_TOKEN ç¯å¢ƒå˜é‡æœªè®¾ç½®")
    print("   è¯·è®¾ç½® Hugging Face Token:")
    print("   export HF_TOKEN=<YOUR_HUGGINGFACE_TOKEN>")
    print("")
else:
    os.environ['HUGGING_FACE_HUB_TOKEN'] = os.environ['HF_TOKEN']
    print(f"âœ“ HF_TOKEN å·²è®¾ç½® (é•¿åº¦: {len(os.environ['HF_TOKEN'])} å­—ç¬¦)")

app = Flask(__name__)
CORS(app)
socketio = SocketIO(app, cors_allowed_origins="*", async_mode='threading')

# å…¨å±€å˜é‡ - æ¨¡å‹çŠ¶æ€
model_state = None
model_lock = threading.Lock()
conversation_active = False  # è·Ÿè¸ªæ˜¯å¦æœ‰æ´»è·ƒå¯¹è¯
last_audio_time = 0  # ä¸Šæ¬¡å¤„ç†éŸ³é¢‘çš„æ—¶é—´

# è‡ªåŠ¨æ£€æµ‹è®¾å¤‡ï¼ˆä¼˜å…ˆ CUDAï¼Œäº‘ç«¯ GPU ä½¿ç”¨ï¼‰
device = "cuda" if torch.cuda.is_available() else ("mps" if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else "cpu")

if torch.cuda.is_available():
    print(f"âœ“ æ£€æµ‹åˆ° CUDA GPU: {torch.cuda.get_device_name(0)}")
    print(f"  GPU å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
else:
    print(f"âš ï¸  æœªæ£€æµ‹åˆ° CUDA GPUï¼Œä½¿ç”¨è®¾å¤‡: {device}")

# å¤„ç†é˜Ÿåˆ—ï¼ˆé™åˆ¶å¹¶å‘ï¼Œé¿å…å†…å­˜æº¢å‡ºï¼‰
processing_queue = queue.Queue(maxsize=1)  # æœ€å¤š1ä¸ªè¯·æ±‚æ’é˜Ÿï¼ˆå‡å°‘åŒæ—¶å›å¤ï¼‰
is_processing = False
pending_requests = set()  # è·Ÿè¸ªæ­£åœ¨å¤„ç†çš„è¯·æ±‚ï¼ˆç”¨äºå»é‡ï¼‰

def clear_memory():
    """æ¸…ç†å†…å­˜"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

def wrap_with_system_tags(text: str) -> str:
    """åŒ…è£…ç³»ç»Ÿæç¤ºè¯"""
    cleaned = text.strip()
    if not cleaned:
        return ""
    return f"<system> {cleaned} <system>"

def warmup(mimi, other_mimi, lm_gen, device, frame_size):
    """é¢„çƒ­æ¨¡å‹"""
    for _ in range(4):
        chunk = torch.zeros(1, 1, frame_size, dtype=torch.float32, device=device)
        codes = mimi.encode(chunk)
        _ = other_mimi.encode(chunk)
        for c in range(codes.shape[-1]):
            tokens = lm_gen.step(codes[:, :, c: c + 1])
            if tokens is None:
                continue
            _ = mimi.decode(tokens[:, 1:9])
            _ = other_mimi.decode(tokens[:, 1:9])
    
    if device == "cuda" or (isinstance(device, torch.device) and device.type == 'cuda'):
        torch.cuda.synchronize()

def load_personaplex_model():
    """åŠ è½½ PersonaPlex æ¨¡å‹å¹¶ä¿æŒçŠ¶æ€"""
    global model_state
    
    try:
        from moshi.models.loaders import get_mimi, get_moshi_lm, MIMI_NAME, TEXT_TOKENIZER_NAME, MOSHI_NAME
        from moshi.models.lm import LMGen
        from moshi.offline import _get_voice_prompt_dir
        from huggingface_hub import hf_hub_download
        
        print(f"æ­£åœ¨åŠ è½½ PersonaPlex æ¨¡å‹...")
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # æ¸…ç†å†…å­˜
        clear_memory()
        
        hf_repo = "nvidia/personaplex-7b-v1"
        hf_token = os.environ.get('HF_TOKEN') or os.environ.get('HUGGING_FACE_HUB_TOKEN')
        
        # ç¡®ä¿ä½¿ç”¨ HF_TOKEN
        if hf_token:
            try:
                from huggingface_hub import login
                login(token=hf_token, add_to_git_credential=False)
            except Exception as e:
                print(f"âš ï¸  ç™»å½• Hugging Face å¤±è´¥: {e}")
        
        # ä¸‹è½½ config.json ä»¥å¢åŠ ä¸‹è½½è®¡æ•°
        hf_hub_download(hf_repo, "config.json", token=hf_token)
        
        # 1) åŠ è½½ Mimi ç¼–ç å™¨/è§£ç å™¨
        print("æ­£åœ¨åŠ è½½ Mimi...")
        mimi_weight = hf_hub_download(hf_repo, MIMI_NAME, token=hf_token)
        mimi = get_mimi(mimi_weight, device)
        other_mimi = get_mimi(mimi_weight, device)
        print("âœ“ Mimi å·²åŠ è½½")
        
        # 2) åŠ è½½ tokenizer
        print("æ­£åœ¨åŠ è½½ tokenizer...")
        tokenizer_path = hf_hub_download(hf_repo, TEXT_TOKENIZER_NAME, token=hf_token)
        text_tokenizer = sentencepiece.SentencePieceProcessor(tokenizer_path)
        print("âœ“ Tokenizer å·²åŠ è½½")
        
        # 3) åŠ è½½ Moshi LM
        print("æ­£åœ¨åŠ è½½ Moshi LM...")
        moshi_weight = hf_hub_download(hf_repo, MOSHI_NAME, token=hf_token)
        use_cpu_offload = False if torch.cuda.is_available() else True
        lm = get_moshi_lm(moshi_weight, device=device, cpu_offload=use_cpu_offload)
        lm.eval()
        print("âœ“ Moshi LM å·²åŠ è½½")
        
        # 4) åˆ›å»º LMGen
        frame_size = int(mimi.sample_rate / mimi.frame_rate)
        lm_gen = LMGen(
            lm,
            audio_silence_frame_cnt=int(0.5 * mimi.frame_rate),
            sample_rate=mimi.sample_rate,
            device=device,
            frame_rate=mimi.frame_rate,
            save_voice_prompt_embeddings=False,
            use_sampling=True,
            temp=0.8,
            temp_text=0.7,
            top_k=250,
            top_k_text=25,
        )
        
        # ä¿æŒæµå¼çŠ¶æ€
        mimi.streaming_forever(1)
        other_mimi.streaming_forever(1)
        lm_gen.streaming_forever(1)
        
        # 5) é¢„çƒ­
        print("æ­£åœ¨é¢„çƒ­æ¨¡å‹...")
        warmup(mimi, other_mimi, lm_gen, device, frame_size)
        print("âœ“ æ¨¡å‹é¢„çƒ­å®Œæˆ")
        
        # è·å– voice prompt ç›®å½•
        voice_prompt_dir = _get_voice_prompt_dir(None, hf_repo)
        
        # ä¿å­˜æ¨¡å‹çŠ¶æ€
        model_state = {
            'mimi': mimi,
            'other_mimi': other_mimi,
            'text_tokenizer': text_tokenizer,
            'lm_gen': lm_gen,
            'device': device,
            'frame_size': frame_size,
            'voice_prompt_dir': voice_prompt_dir,
            'sample_rate': mimi.sample_rate,
        }
        
        print("âœ“ PersonaPlex æ¨¡å‹å·²åŠ è½½å¹¶ä¿æŒçŠ¶æ€")
        return True
        
    except ImportError as e:
        print(f"âœ— æ— æ³•å¯¼å…¥ moshi åŒ…: {e}")
        print("   è¯·å®‰è£…: pip install -e personaplex/moshi/")
        import traceback
        traceback.print_exc()
        return False
    except Exception as e:
        print(f"âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def process_audio_chunk(audio_data, text_prompt, voice_prompt_path=None):
    """å¤„ç†éŸ³é¢‘å— - ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹çŠ¶æ€"""
    global model_state
    
    if model_state is None:
        print("âœ— æ¨¡å‹æœªåŠ è½½")
        return None
    
    # æ£€æŸ¥éŸ³é¢‘é•¿åº¦ï¼ˆé™åˆ¶æœ€å¤§é•¿åº¦ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨ï¼‰
    # å¢åŠ åˆ°10ç§’ï¼Œå…è®¸æ›´é•¿çš„å¥å­
    max_samples = model_state['sample_rate'] * 10  # æœ€å¤š10ç§’
    if len(audio_data) > max_samples:
        print(f"âš ï¸  éŸ³é¢‘å¤ªé•¿ ({len(audio_data)} é‡‡æ ·ç‚¹)ï¼Œæˆªæ–­åˆ° {max_samples}")
        audio_data = audio_data[:max_samples]
    
    # å¤„ç†å‰æ¸…ç† CUDA ç¼“å­˜
    clear_memory()
    
    try:
        with model_lock:
            mimi = model_state['mimi']
            other_mimi = model_state['other_mimi']
            text_tokenizer = model_state['text_tokenizer']
            lm_gen = model_state['lm_gen']
            device = model_state['device']
            frame_size = model_state['frame_size']
            sample_rate = model_state['sample_rate']
            
            global conversation_active, last_audio_time
            current_time = time.time()
            
            # å¦‚æœè·ç¦»ä¸Šæ¬¡å¤„ç†è¶…è¿‡5ç§’ï¼Œè®¤ä¸ºæ˜¯æ–°å¯¹è¯
            is_new_conversation = not conversation_active or (current_time - last_audio_time) > 5.0
            
            if is_new_conversation:
                print("ğŸ”„ å¼€å§‹æ–°å¯¹è¯ï¼Œåˆå§‹åŒ–ç³»ç»Ÿæç¤º...")
                # é‡ç½®æµå¼çŠ¶æ€ï¼ˆå¼€å§‹æ–°å¯¹è¯ï¼‰
                mimi.reset_streaming()
                other_mimi.reset_streaming()
                lm_gen.reset_streaming()
                
                # è®¾ç½® text prompt
                if text_prompt:
                    wrapped_prompt = wrap_with_system_tags(text_prompt)
                    lm_gen.text_prompt_tokens = text_tokenizer.encode(wrapped_prompt) if wrapped_prompt else None
                else:
                    lm_gen.text_prompt_tokens = None
                
                # è®¾ç½® voice prompt
                if voice_prompt_path is None:
                    voice_prompt_dir = model_state['voice_prompt_dir']
                    voice_prompt_path = os.path.join(voice_prompt_dir, "NATF2.pt")
                    if not os.path.exists(voice_prompt_path):
                        # å°è¯•å…¶ä»–è·¯å¾„
                        voice_prompt_path = "NATF2.pt"
                
                if os.path.exists(voice_prompt_path):
                    if voice_prompt_path.endswith('.pt'):
                        lm_gen.load_voice_prompt_embeddings(voice_prompt_path)
                    else:
                        lm_gen.load_voice_prompt(voice_prompt_path)
                
                # è¿è¡Œç³»ç»Ÿæç¤ºé˜¶æ®µï¼ˆåªåœ¨æ–°å¯¹è¯æ—¶è¿è¡Œï¼Œè¿™æ˜¯æœ€è€—æ—¶çš„æ­¥éª¤ï¼‰
                lm_gen.step_system_prompts(mimi)
                mimi.reset_streaming()  # é‡ç½® mimi æµå¼çŠ¶æ€
                conversation_active = True
            else:
                print("â¡ï¸  ç»§ç»­å¯¹è¯ï¼Œè·³è¿‡ç³»ç»Ÿæç¤ºåˆå§‹åŒ–")
                # ç»§ç»­å¯¹è¯ï¼Œåªé‡ç½®æµå¼çŠ¶æ€ï¼Œä¸é‡æ–°è¿è¡Œç³»ç»Ÿæç¤º
                mimi.reset_streaming()
                other_mimi.reset_streaming()
                lm_gen.reset_streaming()
            
            last_audio_time = current_time
            
            print(f"å¼€å§‹å¤„ç†éŸ³é¢‘ï¼ˆ{len(audio_data)} é‡‡æ ·ç‚¹ï¼Œçº¦ {len(audio_data)/sample_rate:.1f} ç§’ï¼‰...")
            start_time = time.time()
            
            # å¤„ç†éŸ³é¢‘å¸§
            generated_frames = []
            # ç¡®ä¿éŸ³é¢‘æ•°æ®æ˜¯ float32
            if audio_data.dtype != np.float32:
                audio_data = audio_data.astype(np.float32)
            
            audio_tensor = torch.from_numpy(audio_data).float()
            if audio_tensor.dim() == 1:
                audio_tensor = audio_tensor.unsqueeze(0)  # (1, T)
            audio_tensor = audio_tensor.to(device)
            
            # å°†éŸ³é¢‘åˆ†æˆå¸§å¹¶å¤„ç†ï¼ˆä¿æŒ float32ï¼‰
            all_pcm_data = audio_tensor[0].cpu().numpy().astype(np.float32)
            del audio_tensor  # é‡Šæ”¾å†…å­˜
            
            frame_count = 0
            while all_pcm_data.shape[-1] >= frame_size:
                chunk = all_pcm_data[:frame_size]
                all_pcm_data = all_pcm_data[frame_size:]
                
                # æ˜ç¡®æŒ‡å®š dtype ä¸º float32
                chunk_tensor = torch.from_numpy(chunk.astype(np.float32)).float().to(device)[None, None]  # (1, 1, frame_size)
                
                # ç¼–ç 
                codes = mimi.encode(chunk_tensor)
                _ = other_mimi.encode(chunk_tensor)
                del chunk_tensor  # é‡Šæ”¾å†…å­˜
                
                # é€æ­¥å¤„ç†æ¯ä¸ª codebook
                for c in range(codes.shape[-1]):
                    tokens = lm_gen.step(codes[:, :, c: c + 1])
                    if tokens is None:
                        continue
                    
                    # è§£ç éŸ³é¢‘
                    pcm = mimi.decode(tokens[:, 1:9])
                    _ = other_mimi.decode(tokens[:, 1:9])
                    pcm = pcm.detach().cpu().numpy()[0, 0]
                    generated_frames.append(pcm)
                    del pcm  # é‡Šæ”¾ GPU å†…å­˜
                
                del codes  # é‡Šæ”¾å†…å­˜
                frame_count += 1
                
                # æ¯å¤„ç†10å¸§æ¸…ç†ä¸€æ¬¡ç¼“å­˜
                if frame_count % 10 == 0:
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    gc.collect()
            
            # å¤„ç†å‰©ä½™çš„éŸ³é¢‘
            if all_pcm_data.shape[-1] > 0:
                # å¡«å……åˆ° frame_sizeï¼ˆç¡®ä¿ float32ï¼‰
                padding = np.zeros(frame_size - all_pcm_data.shape[-1], dtype=np.float32)
                chunk = np.concatenate([all_pcm_data, padding])
                chunk_tensor = torch.from_numpy(chunk.astype(np.float32)).float().to(device)[None, None]
                codes = mimi.encode(chunk_tensor)
                _ = other_mimi.encode(chunk_tensor)
                del chunk_tensor
                for c in range(codes.shape[-1]):
                    tokens = lm_gen.step(codes[:, :, c: c + 1])
                    if tokens is None:
                        continue
                    pcm = mimi.decode(tokens[:, 1:9])
                    _ = other_mimi.decode(tokens[:, 1:9])
                    pcm = pcm.detach().cpu().numpy()[0, 0]
                    generated_frames.append(pcm)
                    del pcm
                del codes
            
            # åˆå¹¶æ‰€æœ‰ç”Ÿæˆçš„å¸§
            if generated_frames:
                output_audio = np.concatenate(generated_frames)
            else:
                output_audio = np.array([], dtype=np.float32)
            
            # æ¸…ç†å†…å­˜
            del generated_frames
            del all_pcm_data
            clear_memory()
            
            elapsed = time.time() - start_time
            print(f"âœ“ å¤„ç†å®Œæˆï¼ˆè€—æ—¶ {elapsed:.1f} ç§’ï¼‰")
            
            return output_audio
            
    except torch.cuda.OutOfMemoryError as e:
        print(f"âœ— GPU å†…å­˜ä¸è¶³: {e}")
        print("æ­£åœ¨æ¸…ç†å†…å­˜...")
        clear_memory()
        # å°è¯•å†æ¬¡æ¸…ç†
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
        return None
    except Exception as e:
        print(f"å¤„ç†é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        clear_memory()
        return None
    finally:
        # æ³¨æ„ï¼šis_processing åœ¨ process_queue ä¸­ç®¡ç†ï¼Œè¿™é‡Œä¸é‡ç½®
        # æœ€åæ¸…ç†ä¸€æ¬¡
        clear_memory()

def process_queue():
    """å¤„ç†é˜Ÿåˆ—ä¸­çš„è¯·æ±‚"""
    global is_processing
    while True:
        try:
            item = processing_queue.get(timeout=30)
            if item is None:
                break
            
            # æ ‡è®°å¼€å§‹å¤„ç†
            is_processing = True
            
            audio_data, text_prompt, source_lang, target_lang, callback = item
            
            # å¤„ç†éŸ³é¢‘
            translated_audio = process_audio_chunk(audio_data, text_prompt)
            
            # å›è°ƒå‘é€ç»“æœ
            if callback:
                callback(translated_audio)
            
            # æ ‡è®°å¤„ç†å®Œæˆ
            is_processing = False
            processing_queue.task_done()
            
            # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¿ç»­å¤„ç†å¤ªå¿«
            time.sleep(0.1)
            
        except queue.Empty:
            continue
        except Exception as e:
            print(f"é˜Ÿåˆ—å¤„ç†é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            is_processing = False

# å¯åŠ¨é˜Ÿåˆ—å¤„ç†çº¿ç¨‹
queue_thread = threading.Thread(target=process_queue, daemon=True)
queue_thread.start()

@app.route('/')
def index():
    return send_file('index.html')

@app.route('/api/status', methods=['GET'])
def get_status():
    cuda_info = {}
    if torch.cuda.is_available():
        cuda_info = {
            'gpu_name': torch.cuda.get_device_name(0),
            'gpu_memory_gb': round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 1)
        }
    
    return jsonify({
        'model_loaded': model_state is not None,
        'device': device,
        'cuda_available': torch.cuda.is_available(),
        'cuda_info': cuda_info,
        'queue_size': processing_queue.qsize(),
        'is_processing': is_processing
    })

@app.route('/api/load_model', methods=['POST'])
def load_model():
    if model_state is not None:
        return jsonify({'success': True, 'message': 'æ¨¡å‹å·²åŠ è½½'})
    
    success = load_personaplex_model()
    if success:
        return jsonify({'success': True, 'message': 'æ¨¡å‹åŠ è½½æˆåŠŸ'})
    else:
        return jsonify({'success': False, 'message': 'æ¨¡å‹åŠ è½½å¤±è´¥'}), 500

@socketio.on('connect')
def handle_connect():
    print('å®¢æˆ·ç«¯å·²è¿æ¥')

@socketio.on('disconnect')
def handle_disconnect():
    print('å®¢æˆ·ç«¯å·²æ–­å¼€')

@socketio.on('audio_chunk')
def handle_audio_chunk(data):
    """å¤„ç†å®æ—¶éŸ³é¢‘å—"""
    try:
        audio_array = data.get('audio')
        source_lang = data.get('source_lang', 'en')
        target_lang = data.get('target_lang', 'zh')
        
        if audio_array is None or not isinstance(audio_array, list) or len(audio_array) == 0:
            print("âš ï¸  æ— æ•ˆçš„éŸ³é¢‘æ•°æ®")
            socketio.emit('audio_error', {'error': 'Invalid audio data'})
            return
        
        if model_state is None:
            print("âš ï¸  æ¨¡å‹æœªåŠ è½½")
            socketio.emit('audio_error', {'error': 'Model not loaded'})
            return
        
        # è½¬æ¢ä¸º bytes
        audio_bytes = bytes(audio_array)
        
        # éªŒè¯æ–‡ä»¶å¤´
        if len(audio_bytes) < 4 or audio_bytes[:4] != b'RIFF':
            print("âš ï¸  ä¸æ˜¯æœ‰æ•ˆçš„ WAV æ–‡ä»¶")
            socketio.emit('audio_error', {'error': 'Invalid WAV file'})
            return
        
        # ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶å¹¶åŠ è½½
        temp_input = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
        temp_input.write(audio_bytes)
        temp_input.close()
        temp_path = temp_input.name
        
        try:
            # æ˜ç¡®æŒ‡å®š dtype ä¸º float32
            audio_data, sr = librosa.load(temp_path, sr=model_state['sample_rate'], dtype=np.float32)
            print(f"æ”¶åˆ°éŸ³é¢‘: {len(audio_data)} é‡‡æ ·ç‚¹ ({len(audio_data)/sr:.1f} ç§’)")
        except Exception as e:
            print(f"éŸ³é¢‘åŠ è½½é”™è¯¯: {e}")
            socketio.emit('audio_error', {'error': f'Audio load error: {str(e)}'})
            return
        finally:
            if os.path.exists(temp_path):
                os.unlink(temp_path)
        
        # æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦å·²æ»¡æˆ–æ­£åœ¨å¤„ç†
        if processing_queue.full() or is_processing:
            print("âš ï¸  æ­£åœ¨å¤„ç†å…¶ä»–è¯·æ±‚ï¼Œè·³è¿‡æ­¤è¯·æ±‚ï¼ˆé¿å…é‡å¤å›å¤ï¼‰")
            socketio.emit('audio_error', {'error': 'Another request is being processed, please wait'})
            return
        
        # åˆ›å»ºæç¤ºè¯ - ç®€å•å¯¹è¯æµ‹è¯•
        text_prompt = "You enjoy having a good conversation."
        
        # å®šä¹‰å›è°ƒå‡½æ•°
        def send_result(translated_audio):
            if translated_audio is not None and len(translated_audio) > 0:
                try:
                    output_temp = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
                    sf.write(output_temp.name, translated_audio, model_state['sample_rate'])
                    output_temp.close()
                    
                    with open(output_temp.name, 'rb') as f:
                        audio_bytes = f.read()
                    os.unlink(output_temp.name)
                    
                    audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')
                    socketio.emit('translated_audio', {'audio': audio_base64})
                    print("âœ“ å·²å‘é€ç¿»è¯‘ç»“æœ")
                except Exception as e:
                    print(f"å‘é€ç»“æœé”™è¯¯: {e}")
                    socketio.emit('audio_error', {'error': f'Failed to send result: {str(e)}'})
            else:
                socketio.emit('audio_error', {'error': 'Translation failed or empty result'})
        
        # æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—
        try:
            processing_queue.put_nowait((audio_data, text_prompt, source_lang, target_lang, send_result))
            print(f"âœ“ å·²æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—ï¼ˆé˜Ÿåˆ—å¤§å°: {processing_queue.qsize()}ï¼‰")
        except queue.Full:
            print("âš ï¸  é˜Ÿåˆ—å·²æ»¡")
            socketio.emit('audio_error', {'error': 'Processing queue is full'})
        
    except Exception as e:
        print(f"å¤„ç†éŸ³é¢‘å—é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        socketio.emit('audio_error', {'error': f'Server error: {str(e)}'})

if __name__ == '__main__':
    import sys
    port = int(sys.argv[1]) if len(sys.argv) > 1 else 5001
    
    print("=" * 60)
    print("PersonaPlex å®æ—¶ç¿»è¯‘ - äº‘ç«¯ GPU ç‰ˆæœ¬ï¼ˆä¿æŒæ¨¡å‹çŠ¶æ€ï¼‰")
    print("=" * 60)
    if torch.cuda.is_available():
        print(f"âœ“ ä½¿ç”¨ CUDA GPU: {torch.cuda.get_device_name(0)}")
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ° CUDA GPUï¼Œå°†ä½¿ç”¨ CPUï¼ˆè¾ƒæ…¢ï¼‰")
    print("=" * 60)
    
    # å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    load_personaplex_model()
    
    print(f"å¯åŠ¨æœåŠ¡å™¨åœ¨ç«¯å£ {port}")
    print("")
    
    socketio.run(app, host='0.0.0.0', port=port, debug=False)


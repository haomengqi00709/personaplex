"""
PersonaPlex å®æ—¶å¯¹è¯ - äº‘ç«¯ GPU ç‰ˆæœ¬ï¼ˆä¿æŒæ¨¡å‹çŠ¶æ€ï¼‰
åœ¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹ä¸€æ¬¡ï¼Œä¿æŒçŠ¶æ€ï¼Œé¿å…æ¯æ¬¡é‡æ–°åŠ è½½
"""

from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from flask_socketio import SocketIO, emit
import torch
import numpy as np
import soundfile as sf
import librosa
import os
import warnings
import tempfile
import base64
import threading
import gc
import queue
import time
import sentencepiece

warnings.filterwarnings("ignore")

# è®¾ç½® PyTorch CUDA å†…å­˜åˆ†é…é…ç½®ï¼Œå‡å°‘å†…å­˜ç¢ç‰‡
os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')

# æ£€æŸ¥å¹¶è®¾ç½® Hugging Face Token
if not os.environ.get('HF_TOKEN'):
    print("âš ï¸  è­¦å‘Š: HF_TOKEN ç¯å¢ƒå˜é‡æœªè®¾ç½®")
    print("   è¯·è®¾ç½® Hugging Face Token:")
    print("   export HF_TOKEN=<YOUR_HUGGINGFACE_TOKEN>")
    print("")
else:
    os.environ['HUGGING_FACE_HUB_TOKEN'] = os.environ['HF_TOKEN']
    print(f"âœ“ HF_TOKEN å·²è®¾ç½® (é•¿åº¦: {len(os.environ['HF_TOKEN'])} å­—ç¬¦)")

app = Flask(__name__)
CORS(app)
# ä¼˜åŒ– Socket.IO é…ç½®ï¼Œå‡å°‘è¿æ¥é—®é¢˜
socketio = SocketIO(
    app, 
    cors_allowed_origins="*", 
    async_mode='threading',
    ping_timeout=60,  # å¢åŠ  ping è¶…æ—¶æ—¶é—´
    ping_interval=25,  # å¢åŠ  ping é—´éš”
    max_http_buffer_size=10*1024*1024,  # 10MB ç¼“å†²åŒº
    logger=False,  # å…³é—­ Socket.IO å†…éƒ¨æ—¥å¿—ï¼ˆå‡å°‘å™ªéŸ³ï¼‰
    engineio_logger=False
)

# å…¨å±€å˜é‡ - æ¨¡å‹çŠ¶æ€
model_state = None
model_lock = threading.Lock()
conversation_active = False  # è·Ÿè¸ªæ˜¯å¦æœ‰æ´»è·ƒå¯¹è¯
last_audio_time = 0  # ä¸Šæ¬¡å¤„ç†éŸ³é¢‘çš„æ—¶é—´

# è°ƒè¯•ç»Ÿè®¡
debug_stats = {
    'total_requests': 0,
    'successful_requests': 0,
    'failed_requests': 0,
    'total_processing_time': 0.0,
    'last_request_time': None,
    'last_processing_time': 0.0,
    'memory_usage_mb': 0.0,
}

# è‡ªåŠ¨æ£€æµ‹è®¾å¤‡ï¼ˆä¼˜å…ˆ CUDAï¼Œäº‘ç«¯ GPU ä½¿ç”¨ï¼‰
device = "cuda" if torch.cuda.is_available() else ("mps" if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else "cpu")

if torch.cuda.is_available():
    print(f"âœ“ æ£€æµ‹åˆ° CUDA GPU: {torch.cuda.get_device_name(0)}")
    print(f"  GPU å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
else:
    print(f"âš ï¸  æœªæ£€æµ‹åˆ° CUDA GPUï¼Œä½¿ç”¨è®¾å¤‡: {device}")

# å¤„ç†é˜Ÿåˆ—ï¼ˆé™åˆ¶å¹¶å‘ï¼Œé¿å…å†…å­˜æº¢å‡ºï¼‰
processing_queue = queue.Queue(maxsize=1)  # æœ€å¤š1ä¸ªè¯·æ±‚æ’é˜Ÿ
is_processing = False
last_request_id = 0  # è¯·æ±‚IDï¼Œç”¨äºå»é‡
pending_request_time = 0  # å¾…å¤„ç†è¯·æ±‚çš„æ—¶é—´æˆ³

def get_memory_usage():
    """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆMBï¼‰"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**2
        reserved = torch.cuda.memory_reserved() / 1024**2
        return {
            'allocated_mb': round(allocated, 2),
            'reserved_mb': round(reserved, 2),
            'free_mb': round((torch.cuda.get_device_properties(0).total_memory / 1024**2) - reserved, 2)
        }
    return {'allocated_mb': 0, 'reserved_mb': 0, 'free_mb': 0}

def clear_memory():
    """æ¸…ç†å†…å­˜"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

def wrap_with_system_tags(text: str) -> str:
    """åŒ…è£…ç³»ç»Ÿæç¤ºè¯"""
    cleaned = text.strip()
    if not cleaned:
        return ""
    return f"<system> {cleaned} <system>"

def warmup(mimi, other_mimi, lm_gen, device, frame_size):
    """é¢„çƒ­æ¨¡å‹"""
    for _ in range(4):
        chunk = torch.zeros(1, 1, frame_size, dtype=torch.float32, device=device)
        codes = mimi.encode(chunk)
        _ = other_mimi.encode(chunk)
        for c in range(codes.shape[-1]):
            tokens = lm_gen.step(codes[:, :, c: c + 1])
            if tokens is None:
                continue
            _ = mimi.decode(tokens[:, 1:9])
            _ = other_mimi.decode(tokens[:, 1:9])
    
    if device == "cuda" or (isinstance(device, torch.device) and device.type == 'cuda'):
        torch.cuda.synchronize()

def load_personaplex_model():
    """åŠ è½½ PersonaPlex æ¨¡å‹å¹¶ä¿æŒçŠ¶æ€"""
    global model_state
    
    try:
        from moshi.models.loaders import get_mimi, get_moshi_lm, MIMI_NAME, TEXT_TOKENIZER_NAME, MOSHI_NAME
        from moshi.models.lm import LMGen
        from moshi.offline import _get_voice_prompt_dir
        from huggingface_hub import hf_hub_download
        
        print(f"æ­£åœ¨åŠ è½½ PersonaPlex æ¨¡å‹...")
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # æ¸…ç†å†…å­˜
        clear_memory()
        
        hf_repo = "nvidia/personaplex-7b-v1"
        hf_token = os.environ.get('HF_TOKEN') or os.environ.get('HUGGING_FACE_HUB_TOKEN')
        
        # ç¡®ä¿ä½¿ç”¨ HF_TOKEN
        if hf_token:
            try:
                from huggingface_hub import login
                login(token=hf_token, add_to_git_credential=False)
            except Exception as e:
                print(f"âš ï¸  ç™»å½• Hugging Face å¤±è´¥: {e}")
        
        # ä¸‹è½½ config.json ä»¥å¢åŠ ä¸‹è½½è®¡æ•°
        hf_hub_download(hf_repo, "config.json", token=hf_token)
        
        # 1) åŠ è½½ Mimi ç¼–ç å™¨/è§£ç å™¨
        print("æ­£åœ¨åŠ è½½ Mimi...")
        mimi_weight = hf_hub_download(hf_repo, MIMI_NAME, token=hf_token)
        mimi = get_mimi(mimi_weight, device)
        other_mimi = get_mimi(mimi_weight, device)
        print("âœ“ Mimi å·²åŠ è½½")
        
        # 2) åŠ è½½ tokenizer
        print("æ­£åœ¨åŠ è½½ tokenizer...")
        tokenizer_path = hf_hub_download(hf_repo, TEXT_TOKENIZER_NAME, token=hf_token)
        text_tokenizer = sentencepiece.SentencePieceProcessor(tokenizer_path)
        print("âœ“ Tokenizer å·²åŠ è½½")
        
        # 3) åŠ è½½ Moshi LM
        print("æ­£åœ¨åŠ è½½ Moshi LM...")
        moshi_weight = hf_hub_download(hf_repo, MOSHI_NAME, token=hf_token)
        use_cpu_offload = False if torch.cuda.is_available() else True
        lm = get_moshi_lm(moshi_weight, device=device, cpu_offload=use_cpu_offload)
        lm.eval()
        print("âœ“ Moshi LM å·²åŠ è½½")
        
        # 4) åˆ›å»º LMGen
        frame_size = int(mimi.sample_rate / mimi.frame_rate)
        lm_gen = LMGen(
            lm,
            audio_silence_frame_cnt=int(0.5 * mimi.frame_rate),
            sample_rate=mimi.sample_rate,
            device=device,
            frame_rate=mimi.frame_rate,
            save_voice_prompt_embeddings=False,
            use_sampling=True,
            temp=0.8,
            temp_text=0.7,
            top_k=250,
            top_k_text=25,
        )
        
        # ä¿æŒæµå¼çŠ¶æ€
        mimi.streaming_forever(1)
        other_mimi.streaming_forever(1)
        lm_gen.streaming_forever(1)
        
        # 5) é¢„çƒ­
        print("æ­£åœ¨é¢„çƒ­æ¨¡å‹...")
        warmup(mimi, other_mimi, lm_gen, device, frame_size)
        print("âœ“ æ¨¡å‹é¢„çƒ­å®Œæˆ")
        
        # è·å– voice prompt ç›®å½•
        voice_prompt_dir = _get_voice_prompt_dir(None, hf_repo)
        
        # ä¿å­˜æ¨¡å‹çŠ¶æ€
        model_state = {
            'mimi': mimi,
            'other_mimi': other_mimi,
            'text_tokenizer': text_tokenizer,
            'lm_gen': lm_gen,
            'device': device,
            'frame_size': frame_size,
            'voice_prompt_dir': voice_prompt_dir,
            'sample_rate': mimi.sample_rate,
        }
        
        print("âœ“ PersonaPlex æ¨¡å‹å·²åŠ è½½å¹¶ä¿æŒçŠ¶æ€")
        return True
        
    except ImportError as e:
        print(f"âœ— æ— æ³•å¯¼å…¥ moshi åŒ…: {e}")
        print("   è¯·å®‰è£…: pip install -e personaplex/moshi/")
        import traceback
        traceback.print_exc()
        return False
    except Exception as e:
        print(f"âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def process_audio_chunk(audio_data, text_prompt, voice_prompt_path=None):
    """å¤„ç†éŸ³é¢‘å— - ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹çŠ¶æ€"""
    global model_state, debug_stats, last_audio_time
    
    request_start_time = time.time()
    debug_stats['total_requests'] += 1
    debug_stats['last_request_time'] = time.strftime('%H:%M:%S')
    
    if model_state is None:
        print("âœ— [ERROR] æ¨¡å‹æœªåŠ è½½")
        debug_stats['failed_requests'] += 1
        return None
    
    # è®°å½•å†…å­˜ä½¿ç”¨
    mem_info = get_memory_usage()
    debug_stats['memory_usage_mb'] = mem_info['allocated_mb']
    print(f"ğŸ“Š [DEBUG] è¯·æ±‚ #{debug_stats['total_requests']} | å†…å­˜: {mem_info['allocated_mb']:.1f}MB / {mem_info['reserved_mb']:.1f}MB | å¯ç”¨: {mem_info['free_mb']:.1f}MB")
    
    # ç§»é™¤éŸ³é¢‘é•¿åº¦é™åˆ¶ï¼Œå…è®¸å®Œæ•´å¤„ç†
    # åªä¿ç•™æç«¯æƒ…å†µçš„å®‰å…¨æ£€æŸ¥ï¼ˆè¶…è¿‡60ç§’å¯èƒ½æ˜¯é”™è¯¯ï¼‰
    max_samples_safety = model_state['sample_rate'] * 60  # å®‰å…¨ä¸Šé™ï¼š60ç§’ï¼ˆé˜²æ­¢æç«¯æƒ…å†µï¼‰
    if len(audio_data) > max_samples_safety:
        print(f"âš ï¸  [WARN] éŸ³é¢‘å¼‚å¸¸é•¿ ({len(audio_data)} é‡‡æ ·ç‚¹ï¼Œ{len(audio_data)/model_state['sample_rate']:.2f}ç§’)ï¼Œå¯èƒ½æ˜¯é”™è¯¯ï¼Œæˆªæ–­åˆ°å®‰å…¨ä¸Šé™ {max_samples_safety} ({max_samples_safety/model_state['sample_rate']:.2f}ç§’)")
        audio_data = audio_data[:max_samples_safety]
    else:
        print(f"âœ“ [AUDIO] éŸ³é¢‘é•¿åº¦: {len(audio_data)} é‡‡æ ·ç‚¹ ({len(audio_data)/model_state['sample_rate']:.2f}ç§’) - å®Œæ•´å¤„ç†")
    
    # å¤„ç†å‰æ¸…ç† CUDA ç¼“å­˜
    clear_memory()
    
    try:
        with model_lock:
            mimi = model_state['mimi']
            other_mimi = model_state['other_mimi']
            text_tokenizer = model_state['text_tokenizer']
            lm_gen = model_state['lm_gen']
            device = model_state['device']
            frame_size = model_state['frame_size']
            sample_rate = model_state['sample_rate']
            
            global conversation_active, last_audio_time
            current_time = time.time()
            
            # å¦‚æœè·ç¦»ä¸Šæ¬¡å¤„ç†è¶…è¿‡30ç§’ï¼Œè®¤ä¸ºæ˜¯æ–°å¯¹è¯ï¼ˆå¤§å¹…å¢åŠ æ—¶é—´çª—å£ï¼Œå‡å°‘é‡æ–°åˆå§‹åŒ–ï¼‰
            time_since_last = current_time - last_audio_time if last_audio_time > 0 else 999
            is_new_conversation = not conversation_active or time_since_last > 30.0
            
            if not is_new_conversation:
                print(f"â±ï¸  [TIME] è·ç¦»ä¸Šæ¬¡è¯·æ±‚: {time_since_last:.1f}ç§’ï¼ˆç»§ç»­å¯¹è¯ï¼Œè·³è¿‡åˆå§‹åŒ–ï¼‰")
            
            if is_new_conversation:
                print(f"ğŸ”„ [NEW_CONV] å¼€å§‹æ–°å¯¹è¯ #{debug_stats['total_requests']}ï¼Œåˆå§‹åŒ–ç³»ç»Ÿæç¤º...")
                init_start = time.time()
                # é‡ç½®æµå¼çŠ¶æ€ï¼ˆå¼€å§‹æ–°å¯¹è¯ï¼‰
                mimi.reset_streaming()
                other_mimi.reset_streaming()
                lm_gen.reset_streaming()
                
                # è®¾ç½® text prompt
                if text_prompt:
                    wrapped_prompt = wrap_with_system_tags(text_prompt)
                    lm_gen.text_prompt_tokens = text_tokenizer.encode(wrapped_prompt) if wrapped_prompt else None
                else:
                    lm_gen.text_prompt_tokens = None
                
                # è®¾ç½® voice prompt
                if voice_prompt_path is None:
                    voice_prompt_dir = model_state['voice_prompt_dir']
                    voice_prompt_path = os.path.join(voice_prompt_dir, "NATF2.pt")
                    if not os.path.exists(voice_prompt_path):
                        # å°è¯•å…¶ä»–è·¯å¾„
                        voice_prompt_path = "NATF2.pt"
                
                if os.path.exists(voice_prompt_path):
                    if voice_prompt_path.endswith('.pt'):
                        lm_gen.load_voice_prompt_embeddings(voice_prompt_path)
                    else:
                        lm_gen.load_voice_prompt(voice_prompt_path)
                
                # è¿è¡Œç³»ç»Ÿæç¤ºé˜¶æ®µï¼ˆåªåœ¨æ–°å¯¹è¯æ—¶è¿è¡Œï¼Œè¿™æ˜¯æœ€è€—æ—¶çš„æ­¥éª¤ï¼‰
                lm_gen.step_system_prompts(mimi)
                mimi.reset_streaming()  # é‡ç½® mimi æµå¼çŠ¶æ€
                conversation_active = True
                init_time = time.time() - init_start
                print(f"âœ“ [INIT] ç³»ç»Ÿæç¤ºåˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {init_time:.2f}ç§’")
            else:
                print(f"â¡ï¸  [CONTINUE] ç»§ç»­å¯¹è¯ #{debug_stats['total_requests']}ï¼Œè·³è¿‡ç³»ç»Ÿæç¤ºåˆå§‹åŒ–ï¼ˆèŠ‚çœçº¦2.3ç§’ï¼‰")
                # ç»§ç»­å¯¹è¯ï¼Œåªé‡ç½®æµå¼çŠ¶æ€ï¼Œä¸é‡æ–°è¿è¡Œç³»ç»Ÿæç¤º
                mimi.reset_streaming()
                other_mimi.reset_streaming()
                lm_gen.reset_streaming()
            
            audio_duration = len(audio_data) / sample_rate
            print(f"ğŸ¤ [AUDIO] å¼€å§‹å¤„ç†éŸ³é¢‘ | é‡‡æ ·ç‚¹: {len(audio_data)} | æ—¶é•¿: {audio_duration:.2f}ç§’")
            start_time = time.time()
            
            # å¤„ç†éŸ³é¢‘å¸§
            generated_frames = []
            # ç¡®ä¿éŸ³é¢‘æ•°æ®æ˜¯ float32
            if audio_data.dtype != np.float32:
                audio_data = audio_data.astype(np.float32)
            
            audio_tensor = torch.from_numpy(audio_data).float()
            if audio_tensor.dim() == 1:
                audio_tensor = audio_tensor.unsqueeze(0)  # (1, T)
            audio_tensor = audio_tensor.to(device)
            
            # å°†éŸ³é¢‘åˆ†æˆå¸§å¹¶å¤„ç†ï¼ˆä¿æŒ float32ï¼‰
            all_pcm_data = audio_tensor[0].cpu().numpy().astype(np.float32)
            del audio_tensor  # é‡Šæ”¾å†…å­˜
            
            frame_count = 0
            encode_time = 0
            decode_time = 0
            step_time = 0
            
            while all_pcm_data.shape[-1] >= frame_size:
                chunk = all_pcm_data[:frame_size]
                all_pcm_data = all_pcm_data[frame_size:]
                
                # æ˜ç¡®æŒ‡å®š dtype ä¸º float32
                chunk_tensor = torch.from_numpy(chunk.astype(np.float32)).float().to(device)[None, None]  # (1, 1, frame_size)
                
                # ç¼–ç 
                encode_start = time.time()
                codes = mimi.encode(chunk_tensor)
                _ = other_mimi.encode(chunk_tensor)
                encode_time += time.time() - encode_start
                del chunk_tensor  # é‡Šæ”¾å†…å­˜
                
                # é€æ­¥å¤„ç†æ¯ä¸ª codebook
                for c in range(codes.shape[-1]):
                    step_start = time.time()
                    tokens = lm_gen.step(codes[:, :, c: c + 1])
                    step_time += time.time() - step_start
                    if tokens is None:
                        continue
                    
                    # è§£ç éŸ³é¢‘
                    decode_start = time.time()
                    pcm = mimi.decode(tokens[:, 1:9])
                    _ = other_mimi.decode(tokens[:, 1:9])
                    decode_time += time.time() - decode_start
                    pcm = pcm.detach().cpu().numpy()[0, 0]
                    generated_frames.append(pcm)
                    del pcm  # é‡Šæ”¾ GPU å†…å­˜
                
                del codes  # é‡Šæ”¾å†…å­˜
                frame_count += 1
                
                # æ¯å¤„ç†10å¸§æ¸…ç†ä¸€æ¬¡ç¼“å­˜
                if frame_count % 10 == 0:
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    gc.collect()
            
            if frame_count > 0:
                print(f"ğŸ“ˆ [PROCESS] å¤„ç†äº† {frame_count} å¸§ | ç¼–ç : {encode_time:.2f}s | æ¨ç†: {step_time:.2f}s | è§£ç : {decode_time:.2f}s")
            
            # å¤„ç†å‰©ä½™çš„éŸ³é¢‘
            if all_pcm_data.shape[-1] > 0:
                # å¡«å……åˆ° frame_sizeï¼ˆç¡®ä¿ float32ï¼‰
                padding = np.zeros(frame_size - all_pcm_data.shape[-1], dtype=np.float32)
                chunk = np.concatenate([all_pcm_data, padding])
                chunk_tensor = torch.from_numpy(chunk.astype(np.float32)).float().to(device)[None, None]
                codes = mimi.encode(chunk_tensor)
                _ = other_mimi.encode(chunk_tensor)
                del chunk_tensor
                for c in range(codes.shape[-1]):
                    tokens = lm_gen.step(codes[:, :, c: c + 1])
                    if tokens is None:
                        continue
                    pcm = mimi.decode(tokens[:, 1:9])
                    _ = other_mimi.decode(tokens[:, 1:9])
                    pcm = pcm.detach().cpu().numpy()[0, 0]
                    generated_frames.append(pcm)
                    del pcm
                del codes
            
            # åˆå¹¶æ‰€æœ‰ç”Ÿæˆçš„å¸§
            if generated_frames:
                output_audio = np.concatenate(generated_frames)
            else:
                output_audio = np.array([], dtype=np.float32)
            
            # æ¸…ç†å†…å­˜
            del generated_frames
            del all_pcm_data
            clear_memory()
            
            elapsed = time.time() - start_time
            total_time = time.time() - request_start_time
            output_duration = len(output_audio) / sample_rate if len(output_audio) > 0 else 0
            
            debug_stats['last_processing_time'] = elapsed
            debug_stats['total_processing_time'] += elapsed
            debug_stats['successful_requests'] += 1
            
            # æ›´æ–°å†…å­˜ä¿¡æ¯
            mem_info = get_memory_usage()
            print(f"âœ“ [DONE] å¤„ç†å®Œæˆ | æ€»è€—æ—¶: {total_time:.2f}s | å¤„ç†è€—æ—¶: {elapsed:.2f}s | è¾“å‡ºæ—¶é•¿: {output_duration:.2f}s")
            print(f"ğŸ“Š [MEMORY] å¤„ç†åå†…å­˜: {mem_info['allocated_mb']:.1f}MB / {mem_info['reserved_mb']:.1f}MB | å¯ç”¨: {mem_info['free_mb']:.1f}MB")
            
            # åœ¨å¤„ç†å®Œæˆåæ›´æ–° last_audio_timeï¼ˆè¿™æ ·ä¸‹æ¬¡è¯·æ±‚æ—¶ï¼Œæ—¶é—´çª—å£æ›´å‡†ç¡®ï¼‰
            last_audio_time = time.time()
            
            return output_audio
            
    except torch.cuda.OutOfMemoryError as e:
        debug_stats['failed_requests'] += 1
        mem_info = get_memory_usage()
        print(f"âœ— [OOM] GPU å†…å­˜ä¸è¶³ | å·²åˆ†é…: {mem_info['allocated_mb']:.1f}MB | å·²ä¿ç•™: {mem_info['reserved_mb']:.1f}MB")
        print(f"   [OOM] é”™è¯¯è¯¦æƒ…: {str(e)[:200]}")
        print("   [OOM] æ­£åœ¨æ¸…ç†å†…å­˜...")
        clear_memory()
        # å°è¯•å†æ¬¡æ¸…ç†
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
        return None
    except Exception as e:
        debug_stats['failed_requests'] += 1
        print(f"âœ— [ERROR] å¤„ç†é”™è¯¯: {type(e).__name__}: {str(e)}")
        import traceback
        traceback.print_exc()
        clear_memory()
        return None
    finally:
        # æ³¨æ„ï¼šis_processing åœ¨ process_queue ä¸­ç®¡ç†ï¼Œè¿™é‡Œä¸é‡ç½®
        # æœ€åæ¸…ç†ä¸€æ¬¡
        clear_memory()

def process_queue():
    """å¤„ç†é˜Ÿåˆ—ä¸­çš„è¯·æ±‚"""
    global is_processing
    while True:
        try:
            item = processing_queue.get(timeout=30)
            if item is None:
                break
            
            # æ ‡è®°å¼€å§‹å¤„ç†
            is_processing = True
            process_start = time.time()
            
            audio_data, text_prompt, source_lang, target_lang, callback = item
            
            # å¤„ç†éŸ³é¢‘
            print(f"ğŸ”„ [QUEUE] å¼€å§‹å¤„ç†é˜Ÿåˆ—ä¸­çš„è¯·æ±‚...")
            response_audio = process_audio_chunk(audio_data, text_prompt)
            
            process_time = time.time() - process_start
            
            # å›è°ƒå‘é€ç»“æœ
            if callback:
                if response_audio is not None and len(response_audio) > 0:
                    print(f"ğŸ“¤ [SEND] å‘é€å“åº”éŸ³é¢‘ï¼Œé•¿åº¦: {len(response_audio)} é‡‡æ ·ç‚¹ | æ€»å¤„ç†æ—¶é—´: {process_time:.2f}ç§’")
                else:
                    print(f"âš ï¸  [SEND] å“åº”éŸ³é¢‘ä¸ºç©ºï¼Œä¸å‘é€")
                callback(response_audio)
            
            # æ ‡è®°å¤„ç†å®Œæˆ
            is_processing = False
            processing_queue.task_done()
            
            # æ¸…ç©ºé˜Ÿåˆ—ä¸­ç­‰å¾…çš„å…¶ä»–è¯·æ±‚ï¼ˆé¿å…å †ç§¯ï¼Œåªå¤„ç†æœ€æ–°çš„ï¼‰
            while not processing_queue.empty():
                try:
                    old_item = processing_queue.get_nowait()
                    print(f"ğŸ—‘ï¸  [CLEAR] ä¸¢å¼ƒé˜Ÿåˆ—ä¸­çš„æ—§è¯·æ±‚ï¼ˆé¿å…å †ç§¯ï¼‰")
                    processing_queue.task_done()
                except queue.Empty:
                    break
            
            # çŸ­æš‚å»¶è¿Ÿ
            time.sleep(0.1)
            
        except queue.Empty:
            continue
        except Exception as e:
            print(f"é˜Ÿåˆ—å¤„ç†é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            is_processing = False

# å¯åŠ¨é˜Ÿåˆ—å¤„ç†çº¿ç¨‹
queue_thread = threading.Thread(target=process_queue, daemon=True)
queue_thread.start()

@app.route('/')
def index():
    return send_file('index.html')

@app.route('/api/status', methods=['GET'])
def get_status():
    cuda_info = {}
    if torch.cuda.is_available():
        cuda_info = {
            'gpu_name': torch.cuda.get_device_name(0),
            'gpu_memory_gb': round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 1)
        }
    
    mem_info = get_memory_usage()
    avg_processing_time = 0.0
    if debug_stats['successful_requests'] > 0:
        avg_processing_time = debug_stats['total_processing_time'] / debug_stats['successful_requests']
    
    return jsonify({
        'model_loaded': model_state is not None,
        'device': device,
        'cuda_available': torch.cuda.is_available(),
        'cuda_info': cuda_info,
        'queue_size': processing_queue.qsize(),
        'is_processing': is_processing,
        'conversation_active': conversation_active,
        'debug_stats': {
            **debug_stats,
            'avg_processing_time': round(avg_processing_time, 2),
            'memory_info': mem_info
        }
    })

@app.route('/api/load_model', methods=['POST'])
def load_model():
    if model_state is not None:
        return jsonify({'success': True, 'message': 'æ¨¡å‹å·²åŠ è½½'})
    
    success = load_personaplex_model()
    if success:
        return jsonify({'success': True, 'message': 'æ¨¡å‹åŠ è½½æˆåŠŸ'})
    else:
        return jsonify({'success': False, 'message': 'æ¨¡å‹åŠ è½½å¤±è´¥'}), 500

@socketio.on('connect')
def handle_connect():
    print(f'ğŸ”Œ [CONNECT] å®¢æˆ·ç«¯å·²è¿æ¥ | æ—¶é—´: {time.strftime("%H:%M:%S")}')

@socketio.on('disconnect')
def handle_disconnect():
    print(f'ğŸ”Œ [DISCONNECT] å®¢æˆ·ç«¯å·²æ–­å¼€ | æ—¶é—´: {time.strftime("%H:%M:%S")}')

@socketio.on('audio_chunk')
def handle_audio_chunk(data):
    """å¤„ç†å®æ—¶éŸ³é¢‘å—"""
    try:
        audio_array = data.get('audio')
        source_lang = data.get('source_lang', 'en')
        target_lang = data.get('target_lang', 'zh')
        
        if audio_array is None or not isinstance(audio_array, list) or len(audio_array) == 0:
            print("âš ï¸  æ— æ•ˆçš„éŸ³é¢‘æ•°æ®")
            socketio.emit('audio_error', {'error': 'Invalid audio data'})
            return
        
        if model_state is None:
            print("âš ï¸  æ¨¡å‹æœªåŠ è½½")
            socketio.emit('audio_error', {'error': 'Model not loaded'})
            return
        
        # è½¬æ¢ä¸º bytes
        audio_bytes = bytes(audio_array)
        
        # éªŒè¯æ–‡ä»¶å¤´
        if len(audio_bytes) < 4 or audio_bytes[:4] != b'RIFF':
            print("âš ï¸  ä¸æ˜¯æœ‰æ•ˆçš„ WAV æ–‡ä»¶")
            socketio.emit('audio_error', {'error': 'Invalid WAV file'})
            return
        
        # ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶å¹¶åŠ è½½
        temp_input = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
        temp_input.write(audio_bytes)
        temp_input.close()
        temp_path = temp_input.name
        
        try:
            # æ˜ç¡®æŒ‡å®š dtype ä¸º float32
            audio_data, sr = librosa.load(temp_path, sr=model_state['sample_rate'], dtype=np.float32)
            audio_duration = len(audio_data) / sr
            print(f"ğŸ“¥ [RECEIVE] æ”¶åˆ°éŸ³é¢‘ | é‡‡æ ·ç‚¹: {len(audio_data)} | æ—¶é•¿: {audio_duration:.2f}ç§’ | æ—¶é—´: {time.strftime('%H:%M:%S')}")
        except Exception as e:
            print(f"éŸ³é¢‘åŠ è½½é”™è¯¯: {e}")
            socketio.emit('audio_error', {'error': f'Audio load error: {str(e)}'})
            return
        finally:
            if os.path.exists(temp_path):
                os.unlink(temp_path)
        
        # æ£€æŸ¥æ˜¯å¦æ­£åœ¨å¤„ç† - å¦‚æœæ­£åœ¨å¤„ç†ï¼Œç›´æ¥ä¸¢å¼ƒæ–°è¯·æ±‚ï¼ˆä¸æ’é˜Ÿï¼Œé¿å…å †ç§¯ï¼‰
        global is_processing, pending_request_time
        current_time = time.time()
        
        if is_processing:
            # å¦‚æœæ­£åœ¨å¤„ç†ï¼Œä¸”è·ç¦»ä¸Šæ¬¡è¯·æ±‚ä¸åˆ°5ç§’ï¼Œç›´æ¥ä¸¢å¼ƒï¼ˆé¿å…å †ç§¯ï¼‰
            if current_time - pending_request_time < 5.0:
                print(f"âš ï¸  [SKIP] æ­£åœ¨å¤„ç†ä¸­ï¼Œä¸¢å¼ƒæ­¤è¯·æ±‚ï¼ˆé¿å…å †ç§¯ï¼‰| è·ç¦»ä¸Šæ¬¡è¯·æ±‚: {current_time - pending_request_time:.1f}ç§’")
                socketio.emit('audio_error', {'error': 'Processing, please wait'})
                return
            else:
                # å¦‚æœå¤„ç†æ—¶é—´å¤ªé•¿ï¼ˆè¶…è¿‡5ç§’ï¼‰ï¼Œå¯èƒ½æ˜¯å¡ä½äº†ï¼Œå…è®¸æ–°è¯·æ±‚
                print(f"âš ï¸  [WARN] å¤„ç†æ—¶é—´è¿‡é•¿ï¼Œå…è®¸æ–°è¯·æ±‚")
        
        # æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦å·²æ»¡
        if processing_queue.full():
            print(f"âš ï¸  [SKIP] é˜Ÿåˆ—å·²æ»¡ï¼Œè·³è¿‡æ­¤è¯·æ±‚")
            socketio.emit('audio_error', {'error': 'Queue is full, please wait'})
            return
        
        pending_request_time = current_time
        
        # åˆ›å»ºæç¤ºè¯ - æ›´æ˜ç¡®çš„å¯¹è¯æŒ‡ä»¤
        text_prompt = "You are a helpful and friendly conversational AI. Respond naturally to what the user says. Do not introduce yourself or say hello unless the user greets you first. Keep your responses concise and relevant to the conversation."
        
        # å®šä¹‰å›è°ƒå‡½æ•°
        def send_result(response_audio):
            send_start = time.time()
            if response_audio is not None and len(response_audio) > 0:
                try:
                    output_temp = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
                    sf.write(output_temp.name, response_audio, model_state['sample_rate'])
                    output_temp.close()
                    
                    with open(output_temp.name, 'rb') as f:
                        audio_bytes = f.read()
                    os.unlink(output_temp.name)
                    
                    audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')
                    response_duration = len(response_audio) / model_state['sample_rate']
                    socketio.emit('translated_audio', {'audio': audio_base64})
                    send_time = time.time() - send_start
                    print(f"âœ“ [SENT] å·²å‘é€å¯¹è¯å“åº” | æ—¶é•¿: {response_duration:.2f}ç§’ | å¤§å°: {len(audio_bytes)} å­—èŠ‚ | å‘é€è€—æ—¶: {send_time:.3f}ç§’")
                except Exception as e:
                    print(f"âœ— [SEND_ERROR] å‘é€ç»“æœé”™è¯¯: {type(e).__name__}: {str(e)}")
                    socketio.emit('audio_error', {'error': f'Failed to send result: {str(e)}'})
            else:
                print(f"âš ï¸  [SEND_ERROR] å“åº”éŸ³é¢‘ä¸ºç©ºï¼Œä¸å‘é€")
                socketio.emit('audio_error', {'error': 'Response failed or empty result'})
        
        # æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—
        try:
            processing_queue.put_nowait((audio_data, text_prompt, source_lang, target_lang, send_result))
            print(f"âœ“ [QUEUE] å·²æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ— | é˜Ÿåˆ—å¤§å°: {processing_queue.qsize()} | ç­‰å¾…å¤„ç†...")
        except queue.Full:
            print(f"âš ï¸  [QUEUE] é˜Ÿåˆ—å·²æ»¡ï¼Œæ— æ³•æ·»åŠ æ–°è¯·æ±‚")
            socketio.emit('audio_error', {'error': 'Processing queue is full'})
        
    except Exception as e:
        print(f"å¤„ç†éŸ³é¢‘å—é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        socketio.emit('audio_error', {'error': f'Server error: {str(e)}'})

if __name__ == '__main__':
    import sys
    port = int(sys.argv[1]) if len(sys.argv) > 1 else 5001
    
    print("=" * 60)
    print("PersonaPlex å®æ—¶å¯¹è¯ - äº‘ç«¯ GPU ç‰ˆæœ¬ï¼ˆä¿æŒæ¨¡å‹çŠ¶æ€ï¼‰")
    print("=" * 60)
    if torch.cuda.is_available():
        print(f"âœ“ ä½¿ç”¨ CUDA GPU: {torch.cuda.get_device_name(0)}")
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ° CUDA GPUï¼Œå°†ä½¿ç”¨ CPUï¼ˆè¾ƒæ…¢ï¼‰")
    print("=" * 60)
    
    # å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    load_personaplex_model()
    
    print(f"å¯åŠ¨æœåŠ¡å™¨åœ¨ç«¯å£ {port}")
    print("")
    
    socketio.run(app, host='0.0.0.0', port=port, debug=False)

